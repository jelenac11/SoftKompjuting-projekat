{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiRJuvlCwqb7"
      },
      "source": [
        "!wget -O archive.zip https://www.dropbox.com/s/5yjv2hth42wbjz3/archive.zip?dl=0\r\n",
        "!unzip \"archive.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOOCejOQRLmO"
      },
      "source": [
        "!pip install tensorlayer\r\n",
        "!pip install emot\r\n",
        "!pip install spellchecker\r\n",
        "!pip install pyspellchecker\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import string \r\n",
        "import nltk\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import re\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from emot.emo_unicode import EMOTICONS\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from torchtext import data\r\n",
        "import tensorflow as tf\r\n",
        "import tensorlayer as tl\r\n",
        "from tensorlayer.cost import cross_entropy_seq_with_mask\r\n",
        "from tensorlayer.models import Seq2seq\r\n",
        "from tqdm import tqdm\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "nltk.download('words')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('stopwords')\r\n",
        "stopwords.words('english')\r\n",
        "%matplotlib inline\r\n",
        "from spellchecker import SpellChecker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCwbPxKNwuoT"
      },
      "source": [
        "def get_response(data, x):\r\n",
        "    \"\"\"\r\n",
        "     Replaces tweet id with its text\r\n",
        "    \"\"\"\r\n",
        "    row = data[data.tweet_id == int(x)]\r\n",
        "    return '' if (len(row) == 0) else row.iloc[0]['text']\r\n",
        "\r\n",
        "df = pd.read_csv(\"twcs/twcs.csv\")\r\n",
        "df.sort_values(by='tweet_id', inplace=True)\r\n",
        "\r\n",
        "support_name = \"AppleSupport\"\r\n",
        "\r\n",
        "support = df[df.author_id == support_name]\r\n",
        "support = support[~support.in_response_to_tweet_id.isna()]\r\n",
        "support = support[['tweet_id', 'author_id', 'text', 'in_response_to_tweet_id']]\r\n",
        "support.loc[:, 'in_response_to_tweet_id'] = support.in_response_to_tweet_id.apply(lambda x: get_response(df, x))\r\n",
        "support.rename(columns = {'author_id':'support_name', 'text':'answer', 'in_response_to_tweet_id':'question'}, inplace = True)\r\n",
        "\r\n",
        "support.to_csv(\"data/\" + support_name + \"-before.csv\", sep='\\t', index=False)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeukAF1Pw2Gc"
      },
      "source": [
        "chat_words = {\r\n",
        "    \"afaik\": \"as far as i know\",\r\n",
        "    \"asap\": \"as soon as possible\",\r\n",
        "    \"atk\": \"at the keyboard\",\r\n",
        "    \"atm\": \"at the moment\",\r\n",
        "    \"brb\": \"be right back\",\r\n",
        "    \"brt\": \"be right there\",\r\n",
        "    \"btw\": \"by the way\",\r\n",
        "    \"b4\": \"before\",\r\n",
        "    \"cu\": \"see you\",\r\n",
        "    \"cya\": \"see you\",\r\n",
        "    \"faq\": \"frequently asked questions\",\r\n",
        "    \"fc\": \"fingers crossed\",\r\n",
        "    \"fyi\": \"for your information\",\r\n",
        "    \"gn\": \"good night\",\r\n",
        "    \"gr8\": \"great\",\r\n",
        "    \"g9\": \"genius\",\r\n",
        "    \"ic\": \"i see\",\r\n",
        "    \"imo\": \"in my opinion\",\r\n",
        "    \"iow\": \"in other words\",\r\n",
        "    \"lol\": \"laughing out loud\",\r\n",
        "    \"l8r\": \"later\",\r\n",
        "    \"mte\": \"my thoughts rxactly\",\r\n",
        "    \"m8\": \"mate\",\r\n",
        "    \"nrn\": \"no reply necessary\",\r\n",
        "    \"oic\": \"oh i see\",\r\n",
        "    \"rofl\": \"rolling on the floor laughing\",\r\n",
        "    \"thx\": \"thank you\",\r\n",
        "    \"ttyl\": \"talk to you later\",\r\n",
        "    \"u\": \"you\",\r\n",
        "    \"u2\": \"you too\",\r\n",
        "    \"w8\": \"wait\",\r\n",
        "    \"imma\": \"i am going to\",\r\n",
        "    \"2nite\": \"tonight\"\r\n",
        "}\r\n",
        "\r\n",
        "contractions = {\r\n",
        "    \"ain't\": \"are not\",\r\n",
        "    \"aren't\": \"are not\",\r\n",
        "    \"'bout\": \"about\",\r\n",
        "    \"can't\": \"cannot\",\r\n",
        "    \"can't've\": \"cannot have\",\r\n",
        "    \"'cause\": \"because\",\r\n",
        "    \"could've\": \"could have\",\r\n",
        "    \"couldn't\": \"could not\",\r\n",
        "    \"couldn't've\": \"could not have\",\r\n",
        "    \"didn't\": \"did not\",\r\n",
        "    \"doesn't\": \"does not\",\r\n",
        "    \"don't\": \"do not\",\r\n",
        "    \"hadn't\": \"had not\",\r\n",
        "    \"hadn't've\": \"had not have\",\r\n",
        "    \"hasn't\": \"has not\",\r\n",
        "    \"haven't\": \"have not\",\r\n",
        "    \"he'd\": \"he would\",\r\n",
        "    \"he'd've\": \"he would have\",\r\n",
        "    \"he'll\": \"he will\",\r\n",
        "    \"he'll've\": \"he will have\",\r\n",
        "    \"he's\": \"he is\",\r\n",
        "    \"here's\": \"here is\",\r\n",
        "    \"how'd\": \"how did\",\r\n",
        "    \"how'd'y\": \"how do you\",\r\n",
        "    \"how'll\": \"how will\",\r\n",
        "    \"how's\": \"how is\",\r\n",
        "    \"i'd\": \"I would\",\r\n",
        "    \"i'd've\": \"I would have\",\r\n",
        "    \"i'll\": \"I will\",\r\n",
        "    \"i'll've\": \"I will have\",\r\n",
        "    \"i'm\": \"I am\",\r\n",
        "    \"i've\": \"I have\",\r\n",
        "    \"isn't\": \"is not\",\r\n",
        "    \"it'd\": \"it would\",\r\n",
        "    \"it'd've\": \"it would have\",\r\n",
        "    \"it'll\": \"it will\",\r\n",
        "    \"it'll've\": \"it will have\",\r\n",
        "    \"it's\": \"it is\",\r\n",
        "    \"i phone\": \"iphone\",\r\n",
        "    \"let's\": \"let us\",\r\n",
        "    \"ma'am\": \"madam\",\r\n",
        "    \"mayn't\": \"may not\",\r\n",
        "    \"might've\": \"might have\",\r\n",
        "    \"mightn't\": \"might not\",\r\n",
        "    \"mightn't've\": \"might not have\",\r\n",
        "    \"must've\": \"must have\",\r\n",
        "    \"mustn't\": \"must not\",\r\n",
        "    \"mustn't've\": \"must not have\",\r\n",
        "    \"needn't\": \"need not\",\r\n",
        "    \"needn't've\": \"need not have\",\r\n",
        "    \"o'clock\": \"of the clock\",\r\n",
        "    \"oughtn't\": \"ought not\",\r\n",
        "    \"oughtn't've\": \"ought not have\",\r\n",
        "    \"shan't\": \"shall not\",\r\n",
        "    \"sha'n't\": \"shall not\",\r\n",
        "    \"shan't've\": \"shall not have\",\r\n",
        "    \"she'd\": \"she would\",\r\n",
        "    \"she'd've\": \"she would have\",\r\n",
        "    \"she'll\": \"she will\",\r\n",
        "    \"she'll've\": \"she will have\",\r\n",
        "    \"she's\": \"she is\",\r\n",
        "    \"should've\": \"should have\",\r\n",
        "    \"shouldn't\": \"should not\",\r\n",
        "    \"shouldn't've\": \"should not have\",\r\n",
        "    \"so've\": \"so have\",\r\n",
        "    \"so's\": \"so is\",\r\n",
        "    \"that'd\": \"that had\",\r\n",
        "    \"that'd've\": \"that would have\",\r\n",
        "    \"that's\": \"that is\",\r\n",
        "    \"there'd\": \"there would\",\r\n",
        "    \"there'd've\": \"there would have\",\r\n",
        "    \"there's\": \"there is\",\r\n",
        "    \"they'd\": \"they would\",\r\n",
        "    \"they'd've\": \"they would have\",\r\n",
        "    \"they'll\": \"they will\",\r\n",
        "    \"they're\": \"they are\",\r\n",
        "    \"they've\": \"they have\",\r\n",
        "    \"'til\": \"until\",\r\n",
        "    \"to've\": \"to have\",\r\n",
        "    \"wasn't\": \"was not\",\r\n",
        "    \"we'd\": \"we would\",\r\n",
        "    \"we'd've\": \"we would have\",\r\n",
        "    \"we'll\": \"we will\",\r\n",
        "    \"we'll've\": \"we will have\",\r\n",
        "    \"we're\": \"we are\",\r\n",
        "    \"we've\": \"we have\",\r\n",
        "    \"weren't\": \"were not\",\r\n",
        "    \"what'll\": \"what will\",\r\n",
        "    \"what're\": \"what are\",\r\n",
        "    \"what's\": \"what is\",\r\n",
        "    \"what've\": \"what have\",\r\n",
        "    \"when's\": \"when is\",\r\n",
        "    \"when've\": \"when have\",\r\n",
        "    \"where'd\": \"where did\",\r\n",
        "    \"where's\": \"where is\",\r\n",
        "    \"where've\": \"where have\",\r\n",
        "    \"who'll\": \"who will\",\r\n",
        "    \"who's\": \"who is\",\r\n",
        "    \"who've\": \"who have\",\r\n",
        "    \"why's\": \"why is\",\r\n",
        "    \"why've\": \"why have\",\r\n",
        "    \"will've\": \"will have\",\r\n",
        "    \"won't\": \"will not\",\r\n",
        "    \"won't've\": \"will not have\",\r\n",
        "    \"would've\": \"would have\",\r\n",
        "    \"wouldn't\": \"would not\",\r\n",
        "    \"wouldn't've\": \"would not have\",\r\n",
        "    \"y'all\": \"you all\",\r\n",
        "    \"y'all'd\": \"you all would\",\r\n",
        "    \"y'all'd've\": \"you all would have\",\r\n",
        "    \"y'all're\": \"you all are\",\r\n",
        "    \"y'all've\": \"you all have\",\r\n",
        "    \"you'll\": \"you will\",\r\n",
        "    \"you're\": \"you are\",\r\n",
        "    \"you've\": \"you have\",\r\n",
        "}\r\n",
        "\r\n",
        "def remove_chat_words_and_contractions(text):\r\n",
        "    \"\"\"\r\n",
        "      Removes chat words listed above and contractions, also listed above from text\r\n",
        "    \"\"\"\r\n",
        "    new_text = []\r\n",
        "    for word in text.split(' '):\r\n",
        "        if word in chat_words.keys():\r\n",
        "            new_text += chat_words[word].split(' ')\r\n",
        "        if word in contractions.keys():\r\n",
        "            new_text += contractions[word].split(' ')\r\n",
        "        else:\r\n",
        "            new_text.append(word)\r\n",
        "    return ' '.join(new_text)\r\n",
        "\r\n",
        "def remove_urls(text):\r\n",
        "    \"\"\"\r\n",
        "      Replaces urls from dataset with url as text\r\n",
        "    \"\"\"\r\n",
        "    return re.sub(r'https://t\\.co/\\w+', 'url', text)\r\n",
        "\r\n",
        "def remove_mentions(text):\r\n",
        "    \"\"\"\r\n",
        "      Removes user mentions from text\r\n",
        "    \"\"\"\r\n",
        "    return re.sub(r'@[0-9A-Za-z_\\-]+', '', text)\r\n",
        "\r\n",
        "def remove_hashtags(text):\r\n",
        "    \"\"\"\r\n",
        "      Removes hashtags\r\n",
        "    \"\"\"\r\n",
        "    return re.sub(r'#[0-9A-Za-z_\\-]+', '', text)\r\n",
        "\r\n",
        "def remove_emojis(text):\r\n",
        "    \"\"\"\r\n",
        "      Removes emojis from text\r\n",
        "    \"\"\"\r\n",
        "    emoji_pattern = re.compile(\"[\"\r\n",
        "                               u\"\\U0001F600-\\U0001F64F\"\r\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"\r\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"\r\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"\r\n",
        "                               u\"\\U00002500-\\U00002BEF\"\r\n",
        "                               u\"\\U00002702-\\U000027B0\"\r\n",
        "                               u\"\\U00002702-\\U000027B0\"\r\n",
        "                               u\"\\U000024C2-\\U0001F251\"\r\n",
        "                               u\"\\U0001f926-\\U0001f937\"\r\n",
        "                               u\"\\U00010000-\\U0010ffff\"\r\n",
        "                               u\"\\u2640-\\u2642\"\r\n",
        "                               u\"\\u2600-\\u2B55\"\r\n",
        "                               u\"\\u200d\"\r\n",
        "                               u\"\\u23cf\"\r\n",
        "                               u\"\\u23e9\"\r\n",
        "                               u\"\\u231a\"\r\n",
        "                               u\"\\ufe0f\"\r\n",
        "                               u\"\\u3030\"\r\n",
        "                               \"]+\", flags=re.UNICODE)\r\n",
        "    return emoji_pattern.sub(r'', text)\r\n",
        "\r\n",
        "def remove_emoticons(text):\r\n",
        "    \"\"\"\r\n",
        "      Removes emoticons from text\r\n",
        "    \"\"\"\r\n",
        "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\r\n",
        "    return emoticon_pattern.sub(r'', text)\r\n",
        "\r\n",
        "from string import digits\r\n",
        "puncList = [\"&gt\",\"amp\",\"%\",'newlinechar',\"~\",\".\",\"\\t\",\"\\n\", \"^\", \"_\", \"*\", \"<\", \">\", \";\", \":\", \"!\", \"?\", \"/\", \"\\\\\", \",\", \"#\", \"@\", \"$\", \"&\", \")\", \"(\", \"\\\"\", \"]\", \"[\", \"|\", \"{\", \"}\",\"=\",\"-\",\"+\",\"\\\"\"]\r\n",
        "isascii = lambda s: len(s) == len(s.encode())\r\n",
        "remove_digits = str.maketrans('', '', digits)\r\n",
        "\r\n",
        "def remove_numbers(content):\r\n",
        "    \"\"\"\r\n",
        "      Removes numbers from text\r\n",
        "    \"\"\"\r\n",
        "    return content.translate(remove_digits)\r\n",
        "\r\n",
        "def isEnglish(s):\r\n",
        "    \"\"\"\r\n",
        "      Checks if s is english word\r\n",
        "    \"\"\"\r\n",
        "    try:\r\n",
        "      s.encode(encoding='utf-8').decode('ascii')\r\n",
        "    except UnicodeDecodeError:\r\n",
        "      return False\r\n",
        "    else:\r\n",
        "      return True\r\n",
        "\r\n",
        "def remove_inerpunction_and_nonascii_chars(content):\r\n",
        "    \"\"\"\r\n",
        "      Removes inerpunction and nonascii characters from content\r\n",
        "    \"\"\"\r\n",
        "    new_content = \"\"\r\n",
        "    content = content.split(\" \")\r\n",
        "    for word in content:\r\n",
        "        word = word.strip()\r\n",
        "        for punc in puncList:\r\n",
        "            word = ''.join(word.split(punc))\r\n",
        "        if word != \"\" and word != \" \" and isEnglish(word):\r\n",
        "            new_content = new_content + \" \" + word\r\n",
        "        if not isEnglish(word):\r\n",
        "            return \"\"\r\n",
        "    return new_content.strip()\r\n",
        "\r\n",
        "def remove_spacing(text):\r\n",
        "    \"\"\"\r\n",
        "      Removes spaces from text\r\n",
        "    \"\"\"\r\n",
        "    return \" \".join([word for word in nltk.word_tokenize(text)])\r\n",
        "\r\n",
        "def clean(text):\r\n",
        "    \"\"\"\r\n",
        "      Cleans text calling functions defined above\r\n",
        "    \"\"\"\r\n",
        "    text = re.sub(r'[“”]', '\"', text)\r\n",
        "    text = re.sub(r\"’\", \"'\", text)\r\n",
        "    text = text.lower()\r\n",
        "    text = remove_chat_words_and_contractions(text)\r\n",
        "    text = remove_urls(text)\r\n",
        "    text = remove_mentions(text)\r\n",
        "    text = remove_hashtags(text)\r\n",
        "    text = re.sub(r'&gt;', '>', text)\r\n",
        "    text = re.sub(r'&lt;', '<', text)\r\n",
        "    text = remove_emojis(text)\r\n",
        "    text = remove_emoticons(text)\r\n",
        "    text = remove_inerpunction_and_nonascii_chars(text)\r\n",
        "    text = remove_numbers(text)\r\n",
        "    text = remove_spacing(text)\r\n",
        "    return text\r\n",
        "\r\n",
        "support.loc[:, 'question'] = support.question.apply(lambda x: clean(x))\r\n",
        "support.loc[:, 'answer'] = support.answer.apply(lambda x: clean(x))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CL2S96RTzl7"
      },
      "source": [
        "spell = SpellChecker()\r\n",
        "\r\n",
        "def spelling_correction(text):\r\n",
        "    \"\"\"\r\n",
        "      Checks if words from text are spelled correctly\r\n",
        "    \"\"\"\r\n",
        "    corrected_text = []\r\n",
        "    misspelled_words = spell.unknown(text.split())    \r\n",
        "    for word in text.split():\r\n",
        "        if word in misspelled_words:\r\n",
        "            corrected_text.append(spell.correction(word))\r\n",
        "        else:\r\n",
        "            corrected_text.append(word)\r\n",
        "    new_text = \" \".join(corrected_text)\r\n",
        "    return new_text\r\n",
        "      \r\n",
        "words = set(nltk.corpus.words.words())\r\n",
        "\r\n",
        "support.loc[:, 'question'] = support.question.apply(lambda x: spelling_correction(x))\r\n",
        "support.loc[:, 'answer'] = support.answer.apply(lambda x: spelling_correction(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqxeel_bVGXz"
      },
      "source": [
        "def get_wordnet_pos(word):\r\n",
        "    \"\"\"\r\n",
        "      Gets the word type\r\n",
        "    \"\"\"\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "few_stopwords = [\"a\", \"the\", \"an\"]\r\n",
        "nltk_stopwords = stopwords.words()\r\n",
        "\r\n",
        "def remove_stopwords(text):\r\n",
        "    \"\"\"\r\n",
        "      Removes english stopwords from text\r\n",
        "    \"\"\"\r\n",
        "    return \" \".join([word for word in nltk.word_tokenize(text) if not word in nltk_stopwords])\r\n",
        "\r\n",
        "def lemmatize(text):\r\n",
        "    \"\"\"\r\n",
        "      Reduces the words(from text) properly ensuring that the root word belongs to the language\r\n",
        "    \"\"\"\r\n",
        "    return \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in nltk.word_tokenize(text)])\r\n",
        "\r\n",
        "def stopwords_and_lemmatize(text):\r\n",
        "    text = remove_stopwords(text)\r\n",
        "    text = lemmatize(text)\r\n",
        "    return text\r\n",
        "\r\n",
        "support.loc[:, 'question'] = support.question.apply(lambda x: stopwords_and_lemmatize(x))\r\n",
        "support.loc[:, 'answer'] = support.answer.apply(lambda x: stopwords_and_lemmatize(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3pHa1EFwE6y"
      },
      "source": [
        "support = support[~(support.answer == '')]\r\n",
        "support = support[~(support.question == '')]\r\n",
        "support = support[~(support.answer == '.')]\r\n",
        "support = support[~(support.question == '.')]\r\n",
        "support = support[support[\"answer\"].str.count(\" \") < 25]\r\n",
        "support = support[support[\"question\"].str.count(\" \") < 25]\r\n",
        "#support = support[support[\"question\"].str.count(\" \") < support[\"answer\"].str.count(\" \") * 1.5]\r\n",
        "#support = support[support[\"answer\"].str.count(\" \") < support[\"question\"].str.count(\" \") * 1.5]\r\n",
        "\r\n",
        "train_set, rest = train_test_split(support, test_size=0.1, random_state=287)\r\n",
        "val_set, test_set = train_test_split(rest, test_size=0.9, random_state=287)\r\n",
        "\r\n",
        "train_set.to_csv('data/' + support_name + '-train.csv', index=False)\r\n",
        "val_set.to_csv('data/' + support_name + '-val.csv', index=False)\r\n",
        "test_set.to_csv('data/' + support_name + '-test.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5titH8Gt8G-X"
      },
      "source": [
        "BATCH_SIZE = 32\r\n",
        "EPOCHS_NUMBER = 40\r\n",
        "VOCABULARY_SIZE = 20000\r\n",
        "EMBEDDING_SIZE = 1024\r\n",
        "DECODER_SEQ_LENGTH = 25 #amazon 20, uber 21\r\n",
        "N_LAYER = 3\r\n",
        "N_UNITS = 256\r\n",
        "LEARNING_RATE = 0.001"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6FZpqV5IjI3"
      },
      "source": [
        "answer_field = data.Field(init_token='<start>', eos_token='<end>', pad_token='<pad>', tokenize='spacy', lower=True, fix_length=DECODER_SEQ_LENGTH)\r\n",
        "question_field = data.Field(init_token='<start>', eos_token='<end>', pad_token='<pad>', tokenize='spacy', lower=True, pad_first=True)\r\n",
        "train, val, test = data.TabularDataset.splits(\r\n",
        "    path='data',\r\n",
        "    format='csv',\r\n",
        "    train=support_name + '-train.csv',\r\n",
        "    validation=support_name + '-val.csv',\r\n",
        "    test=support_name + '-test.csv',\r\n",
        "    fields=[\r\n",
        "        ('tweet_id', None),\r\n",
        "        ('support_name', None),\r\n",
        "        ('answer', answer_field),\r\n",
        "        ('question', question_field)\r\n",
        "    ],\r\n",
        "    skip_header=True\r\n",
        ")\r\n",
        "\r\n",
        "answer_field.build_vocab(train, min_freq=2, max_size=VOCABULARY_SIZE)\r\n",
        "question_field.build_vocab(train, min_freq=2, max_size=VOCABULARY_SIZE)\r\n",
        "answer_field.vocab.stoi['<unk>'] = 1\r\n",
        "answer_field.vocab.stoi['<pad>'] = 0\r\n",
        "question_field.vocab.stoi['<unk>'] = 1\r\n",
        "question_field.vocab.stoi['<pad>'] = 0\r\n",
        "\r\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\r\n",
        "    (train, val, test), batch_size=BATCH_SIZE, sort_key=lambda x: len(x.question), repeat=False, shuffle=True)\r\n",
        "\r\n",
        "vocab = answer_field.vocab"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aU_L9cWiDV0"
      },
      "source": [
        "model = Seq2seq(\r\n",
        "    decoder_seq_length=DECODER_SEQ_LENGTH,\r\n",
        "    cell_enc=tf.keras.layers.LSTMCell,\r\n",
        "    cell_dec=tf.keras.layers.LSTMCell,\r\n",
        "    n_layer=N_LAYER,\r\n",
        "    n_units=N_UNITS,\r\n",
        "    embedding_layer=tl.layers.Embedding(vocabulary_size=VOCABULARY_SIZE, embedding_size=EMBEDDING_SIZE),\r\n",
        ")\r\n",
        "\r\n",
        "def predict(query):\r\n",
        "    \"\"\"\r\n",
        "      Predicts the answer to query\r\n",
        "    \"\"\"\r\n",
        "    with open('data/vocab' + support_name + '.pkl', 'rb') as f:\r\n",
        "        vocab = pickle.load(f)\r\n",
        "    with open('data/invertedVocab' + support_name + '.pkl', 'rb') as f:\r\n",
        "        inverted_vocab = pickle.load(f)\r\n",
        "    model.eval()\r\n",
        "    query = clean(query)\r\n",
        "    new_query = query\r\n",
        "    for el in query.split():\r\n",
        "        if el not in inverted_vocab.keys():\r\n",
        "            new_query = ''.join(new_query.split(el))\r\n",
        "    query = new_query\r\n",
        "    query = ' '.join(query.split())\r\n",
        "    if query != \"\":\r\n",
        "        query_tokenized = [inverted_vocab.get(w) for w in query.split()]\r\n",
        "        answer_tokenized = model(inputs=[[query_tokenized]], seq_length=DECODER_SEQ_LENGTH, start_token=VOCABULARY_SIZE + 1, top_n=1)\r\n",
        "        answer = []\r\n",
        "        for word_token in answer_tokenized[0]:\r\n",
        "            w = vocab[word_token.numpy()]\r\n",
        "            if w == '<end>':\r\n",
        "                break\r\n",
        "            answer = answer + [w]\r\n",
        "        return \" \".join(answer)\r\n",
        "    return \"\"\r\n",
        "\r\n",
        "def train():\r\n",
        "    \"\"\"\r\n",
        "      Training the network\r\n",
        "    \"\"\"\r\n",
        "    optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE)\r\n",
        "    model.train()\r\n",
        "    for epoch in range(EPOCHS_NUMBER):\r\n",
        "        total_loss, n_iter = 0, 0\r\n",
        "        model.train()\r\n",
        "        for batch in tqdm(train_iter, total=train_iter.iterations, desc='Epoch[{}/{}]'.format(epoch + 1, EPOCHS_NUMBER), leave=False):\r\n",
        "            questions = np.array(batch.question).transpose().tolist()\r\n",
        "            answers = np.array(batch.answer).transpose().tolist()\r\n",
        "            target_mask = tl.prepro.sequences_get_mask(answers)\r\n",
        "            with tf.GradientTape() as tape:\r\n",
        "                output = model(inputs = [questions, answers])\r\n",
        "                output = tf.reshape(output, [-1, VOCABULARY_SIZE])\r\n",
        "                loss = cross_entropy_seq_with_mask(logits=output, target_seqs=answers, input_mask=target_mask)\r\n",
        "                grad = tape.gradient(loss, model.all_weights)\r\n",
        "                optimizer.apply_gradients(zip(grad, model.all_weights))\r\n",
        "            \r\n",
        "            total_loss += loss\r\n",
        "            n_iter += 1\r\n",
        "\r\n",
        "        print('Epoch [{}/{}]: loss {:.4f}'.format(epoch + 1, EPOCHS_NUMBER, total_loss / n_iter))\r\n",
        "        tl.files.save_npz(model.all_weights, name='data/model.npz')\r\n",
        "\r\n",
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6f2dUOyMMtU"
      },
      "source": [
        "weights = tl.files.load_npz(name='data/model' + support_name + '.npz')\r\n",
        "tl.files.assign_weights(weights, train_model)\r\n",
        "\r\n",
        "while True:\r\n",
        "    query = input(\">> \")\r\n",
        "    sentence = predict(query)\r\n",
        "    print(\"Support >>\", ' '.join(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8bCmgOTOEGW"
      },
      "source": [
        "weights = tl.files.load_npz(name='data/model' + support_name + '.npz')\r\n",
        "tl.files.assign_weights(weights, train_model)\r\n",
        "\r\n",
        "answers = []\r\n",
        "qs = test_set['question'].to_numpy().tolist()\r\n",
        "\r\n",
        "for test_query in qs:\r\n",
        "    support_answer = predict2(test_query)\r\n",
        "    answers.append(' '.join(support_answer))\r\n",
        "\r\n",
        "test_result = pd.DataFrame({'question': qs, 'answer': answers})\r\n",
        "test_result.to_csv(r'data/test' + support_name + '.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}